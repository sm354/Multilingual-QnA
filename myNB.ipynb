{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a77c3fb",
   "metadata": {},
   "source": [
    "### one time installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a73e60a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gdown --id 1pb7gEkctrVrJA79EAIo7H7nuzD6uV1fW\n",
    "# !gdown --id 1oIeAE9HXXKWPcYa-AZ0ht5ef6sKe_Vh_\n",
    "# !gdown --id 10rAuIDvsYR2yDiCqP7GmYGPc-UmtLbJb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd5a3a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet transformers\n",
    "# !pip install --quiet datasets \n",
    "# !pip install --quiet SentencePiece\n",
    "# !pip install --quiet pytorch-lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b18052",
   "metadata": {},
   "source": [
    "### libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0371301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef223314",
   "metadata": {},
   "source": [
    "### hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f9b6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hyperparameters:\n",
    "    # tokenizer\n",
    "    tokenizer_name = \"deepset/xlm-roberta-large-squad2\" # model_name # CHANGE THIS; TRY XLM-ROBERTA\n",
    "    max_len = 384 # maximum length of context and question in a datapoint\n",
    "    overlap_len = 128 # overlap between two parts of the context when it is split\n",
    "    \n",
    "    # model\n",
    "    model_name = \"deepset/xlm-roberta-large-squad2\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48a6df55",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209a9212",
   "metadata": {},
   "source": [
    "#### remove following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e9d08c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f517c0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad_v2 (/home/shubham/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dfce2e0de814614b614167aa235f71a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = load_dataset(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2086e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    max_length = 384\n",
    "    doc_stride = 128\n",
    "    pad_on_right = True\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3104cba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/shubham/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-1f8f445facbaec6d.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6893964dbcec404381a7742ab515dc3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e98f589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 130319\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 11873\n",
      "    })\n",
      "})\n",
      "['id', 'title', 'context', 'question', 'answers']\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['attention_mask', 'end_positions', 'input_ids', 'start_positions'],\n",
      "        num_rows: 133320\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['attention_mask', 'end_positions', 'input_ids', 'start_positions'],\n",
      "        num_rows: 12360\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(datasets)\n",
    "print(datasets[\"train\"].column_names)\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9150da3f",
   "metadata": {},
   "source": [
    "### data - understanding the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "969fda5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(hyperparams.tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9dc0e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv', encoding='utf-8')\n",
    "# test_df = pd.read_csv('test.csv')\n",
    "# sample_df = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b27f1bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = sklearn.utils.shuffle(train_df, random_state=4).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d9d091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # converting into Squad format\n",
    "# def convert_answers(row):\n",
    "#     return {'answer_start': [row[0]], 'text': [row[1]]}\n",
    "\n",
    "# train_df['answers'] = train_df[['answer_start', 'answer_text']].apply(convert_answers, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af9d2cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.loc[:2]\n",
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99d89caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5179ed725</td>\n",
       "      <td>ரேடியம் (Radium) என்பது Ra என்ற மூலக்கூற்று வா...</td>\n",
       "      <td>ரேடியம் எப்போது கண்டுபிடிக்கப்பட்டது?</td>\n",
       "      <td>1898</td>\n",
       "      <td>975</td>\n",
       "      <td>tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b028b54cf</td>\n",
       "      <td>हेलेन एडम्स केलर (27 जून 1880 - 1 जून 1968) एक...</td>\n",
       "      <td>हेलेन केलर की मृत्यु किस वर्ष में हुई थी?</td>\n",
       "      <td>1968</td>\n",
       "      <td>38</td>\n",
       "      <td>hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86eff66f2</td>\n",
       "      <td>फ़ूड एण्ड ड्रग एडमिनिस्ट्रेशन (FDA या USFDA) स...</td>\n",
       "      <td>खाद्य एवं औषधि प्रशासन का मुख्यालय कहाँ पर है?</td>\n",
       "      <td>सिल्वर स्प्रिंग, मैरीलैंड</td>\n",
       "      <td>1092</td>\n",
       "      <td>hindi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                            context  \\\n",
       "0  5179ed725  ரேடியம் (Radium) என்பது Ra என்ற மூலக்கூற்று வா...   \n",
       "1  b028b54cf  हेलेन एडम्स केलर (27 जून 1880 - 1 जून 1968) एक...   \n",
       "2  86eff66f2  फ़ूड एण्ड ड्रग एडमिनिस्ट्रेशन (FDA या USFDA) स...   \n",
       "\n",
       "                                         question                answer_text  \\\n",
       "0           ரேடியம் எப்போது கண்டுபிடிக்கப்பட்டது?                       1898   \n",
       "1       हेलेन केलर की मृत्यु किस वर्ष में हुई थी?                       1968   \n",
       "2  खाद्य एवं औषधि प्रशासन का मुख्यालय कहाँ पर है?  सिल्वर स्प्रिंग, मैरीलैंड   \n",
       "\n",
       "   answer_start language  \n",
       "0           975    tamil  \n",
       "1            38    hindi  \n",
       "2          1092    hindi  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ef80aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(len(train_df)):\n",
    "#     if train_df.loc[idx,'language'] == \"hindi\" and len(tokenizer(train_df.loc[idx, 'context'], train_df.loc[idx, 'question'])['input_ids']) > hyperparams.max_len:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa4791e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tokenizer(\n",
    "    list(train_df['question']), list(train_df['context']),\n",
    "    max_length=hyperparams.max_len, \n",
    "    truncation='only_second',\n",
    "    stride=hyperparams.overlap_len,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    padding=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2ecb621",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])\n",
      "62 62\n"
     ]
    }
   ],
   "source": [
    "print(out.keys())\n",
    "print(len(out['input_ids']), len(out['offset_mapping']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d011103",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "print(len(out['offset_mapping']))\n",
    "# for i in range(len(out['offset_mapping'])):\n",
    "#     print(len(out.sequence_ids(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee5f04a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> right\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.cls_token, tokenizer.bos_token, tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7aaa628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "print(len(out['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36abbe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer.decode(out['input_ids'][0]))\n",
    "# print(tokenizer.decode(out['input_ids'][1]))\n",
    "# print(tokenizer.decode(out['input_ids'][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4abcb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2] <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "map_x2context_idx = out['overflow_to_sample_mapping']\n",
    "print(map_x2context_idx, type(map_x2context_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10cd45cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_offset = out['offset_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe523d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = random.randint(0, len(out['input_ids']))\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37997b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 384\n"
     ]
    }
   ],
   "source": [
    "x_idx = out['input_ids'][idx]\n",
    "x_context_idx = map_x2context_idx[idx]\n",
    "print(idx, x_context_idx, len(x_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ee55ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_x1_x2 = out.sequence_ids(idx) # sequence ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bdd62b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode(x_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1a74cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1898\n",
      "975\n"
     ]
    }
   ],
   "source": [
    "answer_start = train_df.loc[x_context_idx, 'answer_start']\n",
    "answer = train_df.loc[x_context_idx, 'answer_text']\n",
    "print(answer)\n",
    "print(answer_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "213c81ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_idx = x_idx.index(tokenizer.cls_token_id)\n",
    "cls_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67d570be",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_start_char = train_df.loc[x_context_idx, 'answer_start']\n",
    "y_end_char = train_df.loc[x_context_idx, 'answer_start'] + len(train_df.loc[x_context_idx, 'answer_text']) # note it is one char ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260ae138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babab9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaadf2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7d741c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfa7dee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9eac14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ac9f67d",
   "metadata": {},
   "source": [
    "### Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8de06599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_chaii(data_df, tokenizer):\n",
    "    # prepare_chaii takes in raw data and returns tokenized data \n",
    "    # along with position of first token and last token in the answer_text\n",
    "    \n",
    "    # strip trailing and leading whitespaces in context, question, and (answer_text)?\n",
    "    data_df.loc[:, 'context'] = data_df.loc[:, 'context'].apply(lambda sen : str(sen).strip())\n",
    "    data_df.loc[:, 'question'] = data_df.loc[:, 'question'].apply(lambda sen : str(sen).strip())\n",
    "    data_df.loc[:, 'answer_text'] = data_df.loc[:, 'answer_text'].apply(lambda sen : str(sen).strip())\n",
    "    \n",
    "    data_tok = tokenizer(\n",
    "        list(train_df['question']), list(train_df['context']),\n",
    "        max_length=hyperparams.max_len, \n",
    "        truncation='only_second',\n",
    "        stride=hyperparams.overlap_len,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=True,\n",
    "    )\n",
    "    \n",
    "    # data_df contains original raw data having question, context\n",
    "    # data_tok contains tokenized data, where context might have split into multiple sentences \n",
    "    # data_tok is a dict, containing keys : dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])\n",
    "    # every value is a list, and no tensors here\n",
    "    \n",
    "    # adding two more keys that will contain the position of first token and last token in the answer_text\n",
    "    data_tok['start_positions'], data_tok['end_positions'] = [], []\n",
    "    \n",
    "    n_sents = len(data_tok['input_ids'])\n",
    "    map_id_sent2context = data_tok['overflow_to_sample_mapping'] # id means index! since input_ids means various inputs to the model\n",
    "    map_offsets = data_tok['offset_mapping']\n",
    "    assert len(map_offsets) == len(map_id_sent2context) == n_sents\n",
    "    \n",
    "    for input_id in range(n_sents):\n",
    "        sent = data_tok['input_ids'][input_id]\n",
    "        \n",
    "        # get the answer_start and answer_text for this input_id using the id in data_df\n",
    "        context_id = map_id_sent2context[input_id]\n",
    "        answer_text = data_df.loc[context_id, 'answer_text']\n",
    "        answer_start = data_df.loc[context_id, 'answer_start']\n",
    "        answer_end = answer_start + len(answer_text) # will use this in next code block\n",
    "        \n",
    "        # check whether the answer is present in the current input_id or not using offsets\n",
    "        qn_context_id = data_tok.sequence_ids(input_id)\n",
    "        \n",
    "            # first: get the start_idx_token and end_idx_token of context\n",
    "        start_idx_token = qn_context_id.index(1)\n",
    "        end_idx_token = len(qn_context_id) - qn_context_id[::-1].index(1) - 1\n",
    "        \n",
    "            # second: use the offsets for input_id to find if answer_start and answer_end are inside this chunk of context or not\n",
    "        offset_map = map_offsets[input_id]\n",
    "\n",
    "        if answer_start >= offset_map[start_idx_token][0] and answer_end <= offset_map[end_idx_token][1]:\n",
    "            # now finally get the idx_token for the first and last token in the answer_text\n",
    "            while answer_start >= offset_map[start_idx_token][0] and start_idx_token < len(sent):\n",
    "                start_idx_token += 1\n",
    "            while answer_end <= offset_map[end_idx_token][1]:\n",
    "                end_idx_token -= 1\n",
    "            \n",
    "            data_tok['start_positions'].append(start_idx_token - 1)\n",
    "            data_tok['end_positions'].append(end_idx_token + 1)\n",
    "        \n",
    "        else:\n",
    "            cls_token_idx = sent.index(tokenizer.cls_token_id)\n",
    "            assert cls_token_idx == 0\n",
    "            data_tok['start_positions'].append(0) # cls token index\n",
    "            data_tok['end_positions'].append(0) # cls token index\n",
    "\n",
    "    return data_tok     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6d0c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class chaii_ka_data(Dataset):\n",
    "    def __init__(self, data_df, tokenizer, train=True):\n",
    "        super(chaii_ka_data, self).__init__()\n",
    "        '''\n",
    "            train = True means train, train = False means val set; test = True means test set (without labels)\n",
    "            data_df is the pandas dataframe containing context, question, ...        \n",
    "        '''\n",
    "        \n",
    "        # tokenize data samples context;question, and create new samples if overflow\n",
    "        self.data_tok = prepare_chaii(data_df, tokenizer)\n",
    "    \n",
    "    def __getitem__(self, input_id): # index is input_id as used in prepare_chaii()\n",
    "        # sent = self.data_tok['input_ids'][input_id]\n",
    "        # att_mask = self.data_tok['attention_mask'][input_id]\n",
    "        # offset_map = self.data_tok['offset_mapping'][input_id]\n",
    "        # start_idx_tok = self.data_tok['start_positions'][input_id]\n",
    "        # end_idx_tok = self.data_tok['end_positions'][input_id]\n",
    "        \n",
    "        return {k: torch.tensor(v[input_id], dtype=torch.long) for k,v in self.data_tok.items()}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a11b77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = chaii_ka_data(train_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24accb4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5179ed725</td>\n",
       "      <td>ரேடியம் (Radium) என்பது Ra என்ற மூலக்கூற்று வா...</td>\n",
       "      <td>ரேடியம் எப்போது கண்டுபிடிக்கப்பட்டது?</td>\n",
       "      <td>1898</td>\n",
       "      <td>975</td>\n",
       "      <td>tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b028b54cf</td>\n",
       "      <td>हेलेन एडम्स केलर (27 जून 1880 - 1 जून 1968) एक...</td>\n",
       "      <td>हेलेन केलर की मृत्यु किस वर्ष में हुई थी?</td>\n",
       "      <td>1968</td>\n",
       "      <td>38</td>\n",
       "      <td>hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86eff66f2</td>\n",
       "      <td>फ़ूड एण्ड ड्रग एडमिनिस्ट्रेशन (FDA या USFDA) स...</td>\n",
       "      <td>खाद्य एवं औषधि प्रशासन का मुख्यालय कहाँ पर है?</td>\n",
       "      <td>सिल्वर स्प्रिंग, मैरीलैंड</td>\n",
       "      <td>1092</td>\n",
       "      <td>hindi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                            context  \\\n",
       "0  5179ed725  ரேடியம் (Radium) என்பது Ra என்ற மூலக்கூற்று வா...   \n",
       "1  b028b54cf  हेलेन एडम्स केलर (27 जून 1880 - 1 जून 1968) एक...   \n",
       "2  86eff66f2  फ़ूड एण्ड ड्रग एडमिनिस्ट्रेशन (FDA या USFDA) स...   \n",
       "\n",
       "                                         question                answer_text  \\\n",
       "0           ரேடியம் எப்போது கண்டுபிடிக்கப்பட்டது?                       1898   \n",
       "1       हेलेन केलर की मृत्यु किस वर्ष में हुई थी?                       1968   \n",
       "2  खाद्य एवं औषधि प्रशासन का मुख्यालय कहाँ पर है?  सिल्वर स्प्रिंग, मैरीलैंड   \n",
       "\n",
       "   answer_start language  \n",
       "0           975    tamil  \n",
       "1            38    hindi  \n",
       "2          1092    hindi  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab8f2bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([     0,      6,  31250, 106273,    938, 237805, 142272,  91622,   2798,\n",
       "          59920,     32,      2,      2,      6,  31250, 106273,    938,     15,\n",
       "          12248,  28483,     16,  28191,   2552,  12407,  34148,   4864,   2798,\n",
       "          39639,  35867,      6,  58789, 189641,  23618,   6390, 151421,  55039,\n",
       "          34044, 100128,   3769,   6509,  35097,   3219,  59386,   4548, 199777,\n",
       "              5, 116683, 165380, 136259,  14073, 169604,      5, 116683, 165380,\n",
       "          79471,  35186, 130724, 169604,      5,  59386,   4548,  72307,  31756,\n",
       "           2690,   7827,   3937,  46800,  10540,  60070,   9000,  36242,  24183,\n",
       "            116, 130130, 173021,  35097,   6001,   9823, 127183,  59386, 127076,\n",
       "              6,  31250, 106273,    938, 169604,      5,  12751, 114319,  25650,\n",
       "          12359,  66705,  22727, 105971,  43716,  78884,  68960,   3686,   8182,\n",
       "              5,  46018,  12784, 155012,      6,  31250, 106273,    938, 197508,\n",
       "         127316,  45728,  22162,  25650,   7539, 147512,   4548,  74149,  28108,\n",
       "          49604,      5,  28769,  12751,  16043,   8938,  45741,  26415, 176626,\n",
       "           5966,  16242,      6,  31250, 106273,    938,   6001,   1962,   8040,\n",
       "           9734,  64432,   2025,  33676, 115433,  10860,  10106,   5270,  94200,\n",
       "           4930,  48068, 234198,      6, 192992, 129213,  12145,      6,  31250,\n",
       "         106273,    938,  10860,  10106,   5270,  12309,   9000,     15,  12248,\n",
       "            363,    839,  10461,  12407,  39311,   6343, 147512, 177292, 108471,\n",
       "           6343,  81700, 188777, 221659, 128831,      5,      6,  31250, 106273,\n",
       "          21651,  60039,  27705, 162320, 123555,   6343,  28285,  65540,  12009,\n",
       "           6390,   5944,  35424,   2798,   3093, 228988,   2782,  74149,   4930,\n",
       "          51373,   4167,      5,   7216, 125956,   8938,      6,  31250, 106273,\n",
       "            938, 122297,   4046,  12407,  27705, 162320, 123555,   6343,  65540,\n",
       "          55645,   6343,   3770,   2913,   7539,  23618,  28108,  49604,      5,\n",
       "           7216,  75185,  74149, 178304,   6001,  35846,  20539, 142113,  48463,\n",
       "          32105,  91585,   4167,      5,   6390,   5944,  35424,   2798,   6149,\n",
       "          11674,  13945,   3937,  18805,  12145,   5894,  27705, 162320, 123555,\n",
       "           6343,      6,  31250,  35673,   2913,  25856,  35846,  51373,      4,\n",
       "              6,  79464,  20663,   2650,      6,  31250,  35673,   2913,     20,\n",
       "          97400,  12407,  27705, 162320, 123555,  20663,   2650, 144838,  58594,\n",
       "              5,      6,  31250, 106273,    938,  11674,  13945,   3937,  87783,\n",
       "         111445,   2690,   2782, 199464,  14097,   6390, 151421,   3219, 196028,\n",
       "           6509,  76335,  57836,  97861,      5,  12212, 141644,  56150,  24631,\n",
       "           5944,   7114,  76335,   5270,   6819, 132335,   2912,   3937,  39639,\n",
       "          10479,   6390,   5944,  35424,   2798, 141644,   2912,   7065,   3093,\n",
       "          34326,  58594,      5,      6,  31250, 106273,    938,  14233, 178332,\n",
       "          12309,   9000, 147383,   5055,      6,  31250, 106273,    938, 117752,\n",
       "          83505,  32105,  35576,   6016,  16891,  46670,   6016,  13128,   8838,\n",
       "           7016,   6016,  16891,  46670,   6016,      6,  27756,  67313,   2782,\n",
       "          10890,   3769,  86880,  75831,  59920,      5,  25868, 135849, 161902,\n",
       "          98072,  25089,  11449,   3791,      6,  35846,  31250,   9982,  11993,\n",
       "          12407,   6390,  10370,   4548, 211926,      2]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'offset_mapping': tensor([[   0,    0],\n",
       "         [   0,    1],\n",
       "         [   0,    2],\n",
       "         [   2,    5],\n",
       "         [   5,    7],\n",
       "         [   7,   15],\n",
       "         [  15,   21],\n",
       "         [  21,   25],\n",
       "         [  25,   28],\n",
       "         [  28,   36],\n",
       "         [  36,   37],\n",
       "         [   0,    0],\n",
       "         [   0,    0],\n",
       "         [   0,    1],\n",
       "         [   0,    2],\n",
       "         [   2,    5],\n",
       "         [   5,    7],\n",
       "         [   7,    9],\n",
       "         [   9,   11],\n",
       "         [  11,   15],\n",
       "         [  15,   16],\n",
       "         [  16,   23],\n",
       "         [  23,   26],\n",
       "         [  26,   31],\n",
       "         [  31,   34],\n",
       "         [  34,   35],\n",
       "         [  35,   38],\n",
       "         [  38,   39],\n",
       "         [  39,   43],\n",
       "         [  43,   44],\n",
       "         [  44,   48],\n",
       "         [  48,   54],\n",
       "         [  54,   60],\n",
       "         [  61,   63],\n",
       "         [  63,   67],\n",
       "         [  67,   69],\n",
       "         [  69,   73],\n",
       "         [  73,   76],\n",
       "         [  76,   78],\n",
       "         [  78,   79],\n",
       "         [  79,   83],\n",
       "         [  83,   87],\n",
       "         [  87,   91],\n",
       "         [  91,   92],\n",
       "         [  92,   98],\n",
       "         [  98,   99],\n",
       "         [  99,  104],\n",
       "         [ 104,  108],\n",
       "         [ 108,  112],\n",
       "         [ 112,  115],\n",
       "         [ 115,  121],\n",
       "         [ 121,  122],\n",
       "         [ 122,  127],\n",
       "         [ 127,  131],\n",
       "         [ 131,  133],\n",
       "         [ 133,  135],\n",
       "         [ 135,  139],\n",
       "         [ 139,  145],\n",
       "         [ 145,  146],\n",
       "         [ 146,  150],\n",
       "         [ 150,  151],\n",
       "         [ 151,  155],\n",
       "         [ 155,  157],\n",
       "         [ 157,  159],\n",
       "         [ 159,  162],\n",
       "         [ 162,  163],\n",
       "         [ 163,  165],\n",
       "         [ 165,  169],\n",
       "         [ 170,  173],\n",
       "         [ 173,  175],\n",
       "         [ 175,  179],\n",
       "         [ 179,  181],\n",
       "         [ 181,  183],\n",
       "         [ 183,  187],\n",
       "         [ 187,  197],\n",
       "         [ 197,  201],\n",
       "         [ 201,  203],\n",
       "         [ 203,  204],\n",
       "         [ 204,  208],\n",
       "         [ 208,  212],\n",
       "         [ 212,  215],\n",
       "         [ 215,  216],\n",
       "         [ 216,  218],\n",
       "         [ 218,  221],\n",
       "         [ 221,  223],\n",
       "         [ 223,  229],\n",
       "         [ 229,  230],\n",
       "         [ 230,  233],\n",
       "         [ 233,  235],\n",
       "         [ 235,  237],\n",
       "         [ 237,  239],\n",
       "         [ 239,  241],\n",
       "         [ 241,  244],\n",
       "         [ 244,  252],\n",
       "         [ 252,  256],\n",
       "         [ 256,  260],\n",
       "         [ 260,  268],\n",
       "         [ 268,  269],\n",
       "         [ 269,  272],\n",
       "         [ 272,  273],\n",
       "         [ 273,  276],\n",
       "         [ 276,  278],\n",
       "         [ 278,  283],\n",
       "         [ 283,  284],\n",
       "         [ 284,  286],\n",
       "         [ 286,  289],\n",
       "         [ 289,  291],\n",
       "         [ 291,  298],\n",
       "         [ 298,  302],\n",
       "         [ 302,  306],\n",
       "         [ 306,  309],\n",
       "         [ 309,  311],\n",
       "         [ 311,  313],\n",
       "         [ 313,  317],\n",
       "         [ 317,  318],\n",
       "         [ 318,  322],\n",
       "         [ 322,  325],\n",
       "         [ 325,  332],\n",
       "         [ 332,  333],\n",
       "         [ 333,  339],\n",
       "         [ 339,  342],\n",
       "         [ 342,  345],\n",
       "         [ 345,  348],\n",
       "         [ 348,  353],\n",
       "         [ 353,  357],\n",
       "         [ 357,  362],\n",
       "         [ 362,  365],\n",
       "         [ 365,  368],\n",
       "         [ 368,  369],\n",
       "         [ 369,  371],\n",
       "         [ 371,  374],\n",
       "         [ 374,  376],\n",
       "         [ 376,  378],\n",
       "         [ 378,  380],\n",
       "         [ 380,  382],\n",
       "         [ 382,  383],\n",
       "         [ 383,  389],\n",
       "         [ 389,  391],\n",
       "         [ 391,  395],\n",
       "         [ 395,  398],\n",
       "         [ 398,  400],\n",
       "         [ 400,  401],\n",
       "         [ 401,  403],\n",
       "         [ 403,  405],\n",
       "         [ 405,  406],\n",
       "         [ 406,  410],\n",
       "         [ 410,  419],\n",
       "         [ 419,  420],\n",
       "         [ 420,  424],\n",
       "         [ 424,  428],\n",
       "         [ 428,  432],\n",
       "         [ 432,  433],\n",
       "         [ 433,  435],\n",
       "         [ 435,  438],\n",
       "         [ 438,  440],\n",
       "         [ 440,  442],\n",
       "         [ 442,  443],\n",
       "         [ 443,  445],\n",
       "         [ 445,  447],\n",
       "         [ 447,  449],\n",
       "         [ 449,  451],\n",
       "         [ 451,  453],\n",
       "         [ 453,  454],\n",
       "         [ 454,  455],\n",
       "         [ 455,  457],\n",
       "         [ 457,  462],\n",
       "         [ 462,  466],\n",
       "         [ 466,  470],\n",
       "         [ 470,  474],\n",
       "         [ 474,  479],\n",
       "         [ 479,  481],\n",
       "         [ 481,  485],\n",
       "         [ 485,  488],\n",
       "         [ 488,  494],\n",
       "         [ 494,  501],\n",
       "         [ 501,  505],\n",
       "         [ 505,  506],\n",
       "         [ 506,  507],\n",
       "         [ 507,  509],\n",
       "         [ 509,  512],\n",
       "         [ 512,  518],\n",
       "         [ 518,  526],\n",
       "         [ 526,  528],\n",
       "         [ 528,  530],\n",
       "         [ 530,  532],\n",
       "         [ 532,  536],\n",
       "         [ 536,  541],\n",
       "         [ 541,  546],\n",
       "         [ 546,  549],\n",
       "         [ 549,  551],\n",
       "         [ 551,  553],\n",
       "         [ 553,  556],\n",
       "         [ 556,  559],\n",
       "         [ 559,  561],\n",
       "         [ 561,  567],\n",
       "         [ 567,  568],\n",
       "         [ 568,  572],\n",
       "         [ 572,  573],\n",
       "         [ 573,  576],\n",
       "         [ 576,  579],\n",
       "         [ 579,  580],\n",
       "         [ 580,  582],\n",
       "         [ 582,  586],\n",
       "         [ 586,  589],\n",
       "         [ 589,  590],\n",
       "         [ 590,  592],\n",
       "         [ 592,  595],\n",
       "         [ 595,  597],\n",
       "         [ 597,  600],\n",
       "         [ 600,  602],\n",
       "         [ 602,  607],\n",
       "         [ 607,  609],\n",
       "         [ 609,  611],\n",
       "         [ 611,  613],\n",
       "         [ 613,  617],\n",
       "         [ 617,  622],\n",
       "         [ 622,  627],\n",
       "         [ 627,  631],\n",
       "         [ 631,  634],\n",
       "         [ 634,  636],\n",
       "         [ 636,  638],\n",
       "         [ 638,  644],\n",
       "         [ 644,  647],\n",
       "         [ 647,  654],\n",
       "         [ 654,  655],\n",
       "         [ 655,  657],\n",
       "         [ 657,  659],\n",
       "         [ 659,  663],\n",
       "         [ 663,  667],\n",
       "         [ 667,  669],\n",
       "         [ 669,  671],\n",
       "         [ 671,  673],\n",
       "         [ 673,  679],\n",
       "         [ 679,  684],\n",
       "         [ 684,  690],\n",
       "         [ 690,  694],\n",
       "         [ 694,  697],\n",
       "         [ 697,  698],\n",
       "         [ 698,  700],\n",
       "         [ 700,  702],\n",
       "         [ 702,  705],\n",
       "         [ 705,  708],\n",
       "         [ 708,  710],\n",
       "         [ 710,  713],\n",
       "         [ 713,  715],\n",
       "         [ 715,  716],\n",
       "         [ 716,  718],\n",
       "         [ 718,  722],\n",
       "         [ 722,  727],\n",
       "         [ 727,  729],\n",
       "         [ 729,  731],\n",
       "         [ 731,  733],\n",
       "         [ 733,  737],\n",
       "         [ 737,  738],\n",
       "         [ 738,  740],\n",
       "         [ 740,  742],\n",
       "         [ 742,  744],\n",
       "         [ 744,  747],\n",
       "         [ 747,  749],\n",
       "         [ 749,  752],\n",
       "         [ 752,  753],\n",
       "         [ 753,  754],\n",
       "         [ 754,  758],\n",
       "         [ 758,  762],\n",
       "         [ 762,  763],\n",
       "         [ 763,  764],\n",
       "         [ 764,  766],\n",
       "         [ 766,  768],\n",
       "         [ 768,  770],\n",
       "         [ 770,  772],\n",
       "         [ 772,  776],\n",
       "         [ 776,  781],\n",
       "         [ 781,  783],\n",
       "         [ 783,  785],\n",
       "         [ 785,  787],\n",
       "         [ 787,  791],\n",
       "         [ 791,  792],\n",
       "         [ 792,  796],\n",
       "         [ 796,  802],\n",
       "         [ 802,  803],\n",
       "         [ 804,  805],\n",
       "         [ 805,  807],\n",
       "         [ 807,  810],\n",
       "         [ 810,  812],\n",
       "         [ 812,  815],\n",
       "         [ 815,  817],\n",
       "         [ 817,  818],\n",
       "         [ 818,  821],\n",
       "         [ 821,  828],\n",
       "         [ 828,  830],\n",
       "         [ 830,  831],\n",
       "         [ 831,  835],\n",
       "         [ 835,  841],\n",
       "         [ 841,  843],\n",
       "         [ 843,  847],\n",
       "         [ 847,  851],\n",
       "         [ 851,  856],\n",
       "         [ 856,  857],\n",
       "         [ 857,  860],\n",
       "         [ 860,  862],\n",
       "         [ 862,  866],\n",
       "         [ 866,  867],\n",
       "         [ 867,  871],\n",
       "         [ 871,  875],\n",
       "         [ 875,  879],\n",
       "         [ 879,  882],\n",
       "         [ 882,  884],\n",
       "         [ 884,  887],\n",
       "         [ 887,  890],\n",
       "         [ 890,  892],\n",
       "         [ 892,  895],\n",
       "         [ 895,  899],\n",
       "         [ 899,  901],\n",
       "         [ 901,  902],\n",
       "         [ 902,  903],\n",
       "         [ 903,  907],\n",
       "         [ 907,  909],\n",
       "         [ 909,  911],\n",
       "         [ 911,  914],\n",
       "         [ 914,  917],\n",
       "         [ 917,  921],\n",
       "         [ 921,  923],\n",
       "         [ 923,  925],\n",
       "         [ 925,  927],\n",
       "         [ 927,  930],\n",
       "         [ 930,  936],\n",
       "         [ 936,  937],\n",
       "         [ 937,  938],\n",
       "         [ 938,  940],\n",
       "         [ 940,  943],\n",
       "         [ 943,  945],\n",
       "         [ 945,  948],\n",
       "         [ 948,  950],\n",
       "         [ 950,  952],\n",
       "         [ 952,  954],\n",
       "         [ 954,  959],\n",
       "         [ 959,  965],\n",
       "         [ 965,  966],\n",
       "         [ 966,  968],\n",
       "         [ 968,  971],\n",
       "         [ 971,  973],\n",
       "         [ 974,  979],\n",
       "         [ 979,  983],\n",
       "         [ 983,  989],\n",
       "         [ 989,  992],\n",
       "         [ 992,  994],\n",
       "         [ 994,  996],\n",
       "         [ 996,  998],\n",
       "         [ 998, 1000],\n",
       "         [1000, 1008],\n",
       "         [1008, 1010],\n",
       "         [1010, 1012],\n",
       "         [1012, 1014],\n",
       "         [1014, 1016],\n",
       "         [1016, 1018],\n",
       "         [1018, 1020],\n",
       "         [1020, 1021],\n",
       "         [1021, 1024],\n",
       "         [1024, 1027],\n",
       "         [1027, 1028],\n",
       "         [1028, 1030],\n",
       "         [1030, 1032],\n",
       "         [1032, 1037],\n",
       "         [1037, 1040],\n",
       "         [1040, 1048],\n",
       "         [1048, 1049],\n",
       "         [1049, 1053],\n",
       "         [1053, 1057],\n",
       "         [1057, 1066],\n",
       "         [1066, 1069],\n",
       "         [1069, 1071],\n",
       "         [1071, 1073],\n",
       "         [1073, 1077],\n",
       "         [1078, 1079],\n",
       "         [1079, 1081],\n",
       "         [1081, 1083],\n",
       "         [1083, 1085],\n",
       "         [1085, 1089],\n",
       "         [1089, 1094],\n",
       "         [1094, 1096],\n",
       "         [1096, 1098],\n",
       "         [1098, 1099],\n",
       "         [1099, 1111],\n",
       "         [   0,    0]]),\n",
       " 'overflow_to_sample_mapping': tensor(0),\n",
       " 'start_positions': tensor(341),\n",
       " 'end_positions': tensor(341)}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a831dce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a0dc2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55c1394a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = [(1,2.1)]\n",
    "# torch.tensor(a, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3970894d",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6082500c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a4369d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4604cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac481e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed2c87f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd2dc2a2",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad261610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59167fed0aba47abb447a2f411b94303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(hyperparams.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af9a1cc",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad3a35f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b368850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fc0ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5773e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aff96daa",
   "metadata": {},
   "source": [
    "### references\n",
    "1. https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb\n",
    "2. https://huggingface.co/transformers/internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.__call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4795681b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b71625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
