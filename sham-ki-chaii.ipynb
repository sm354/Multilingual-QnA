{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one time installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T05:51:45.938535Z",
     "iopub.status.busy": "2021-11-12T05:51:45.937945Z",
     "iopub.status.idle": "2021-11-12T05:51:45.943323Z",
     "shell.execute_reply": "2021-11-12T05:51:45.942089Z",
     "shell.execute_reply.started": "2021-11-12T05:51:45.938499Z"
    }
   },
   "outputs": [],
   "source": [
    "# !gdown --id 1pb7gEkctrVrJA79EAIo7H7nuzD6uV1fW\n",
    "# !gdown --id 1oIeAE9HXXKWPcYa-AZ0ht5ef6sKe_Vh_\n",
    "# !gdown --id 10rAuIDvsYR2yDiCqP7GmYGPc-UmtLbJb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T05:51:45.951517Z",
     "iopub.status.busy": "2021-11-12T05:51:45.950127Z",
     "iopub.status.idle": "2021-11-12T05:51:45.95595Z",
     "shell.execute_reply": "2021-11-12T05:51:45.954721Z",
     "shell.execute_reply.started": "2021-11-12T05:51:45.951469Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install --quiet transformers --target=/kaggle/working/chaii_packages\n",
    "# !pip install --quiet datasets --target=/kaggle/working/chaii_packages\n",
    "# !pip install --quiet SentencePiece --target=/kaggle/working/chaii_packages\n",
    "# !pip install --quiet pytorch-lightning --target=/kaggle/working/chaii_packages \n",
    "# !pip install ipdb --target=/kaggle/working/chaii_packages\n",
    "# import sys\n",
    "# sys.path.append('/kaggle/working/chaii_packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T05:51:46.891626Z",
     "iopub.status.busy": "2021-11-12T05:51:46.891309Z",
     "iopub.status.idle": "2021-11-12T05:51:46.904536Z",
     "shell.execute_reply": "2021-11-12T05:51:46.90353Z",
     "shell.execute_reply.started": "2021-11-12T05:51:46.891593Z"
    }
   },
   "outputs": [],
   "source": [
    "# %env PYTHONPATH= \n",
    "# %env WANDB_DISABLED=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipdb import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T07:08:43.748452Z",
     "iopub.status.busy": "2021-11-12T07:08:43.74811Z",
     "iopub.status.idle": "2021-11-12T07:08:43.759813Z",
     "shell.execute_reply": "2021-11-12T07:08:43.75753Z",
     "shell.execute_reply.started": "2021-11-12T07:08:43.748421Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import random\n",
    "from sklearn import model_selection\n",
    "# from ipdb import set_trace\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T06:11:15.953146Z",
     "iopub.status.busy": "2021-11-12T06:11:15.952859Z",
     "iopub.status.idle": "2021-11-12T06:11:15.964966Z",
     "shell.execute_reply": "2021-11-12T06:11:15.962542Z",
     "shell.execute_reply.started": "2021-11-12T06:11:15.953113Z"
    }
   },
   "outputs": [],
   "source": [
    "class hyperparameters:\n",
    "    # seed\n",
    "    seed = 4\n",
    "    \n",
    "    # tokenizer\n",
    "    tokenizer_name = \"deepset/xlm-roberta-base-squad2\" # \"deepset/xlm-roberta-large-squad2\" # model_name # CHANGE THIS; TRY XLM-ROBERTA\n",
    "    max_len = 384 # maximum length of context and question in a datapoint\n",
    "    overlap_len = 128 # overlap between two parts of the context when it is split\n",
    "    \n",
    "    # model\n",
    "    model_name = \"deepset/xlm-roberta-base-squad2\" # \"deepset/xlm-roberta-large-squad2\"\n",
    "    batch_size = 8\n",
    "    epochs = 5\n",
    "    \n",
    "    # data\n",
    "    train_csv = \"train.csv\" # \"../input/chaii-hindi-and-tamil-question-answering/train.csv\" \n",
    "    test_csv = \"test.csv\" # \"../input/chaii-hindi-and-tamil-question-answering/test.csv\"\n",
    "    \n",
    "    # prediction\n",
    "    top_x = 5 # top 5 answer predictions by each feature\n",
    "    max_tok_in_ans = 10 # max 10 tokens in predicted answer text\n",
    "    output_dir = \"xlm-chaii\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T06:11:31.70305Z",
     "iopub.status.busy": "2021-11-12T06:11:31.702656Z",
     "iopub.status.idle": "2021-11-12T06:11:31.724828Z",
     "shell.execute_reply": "2021-11-12T06:11:31.720412Z",
     "shell.execute_reply.started": "2021-11-12T06:11:31.703009Z"
    }
   },
   "outputs": [],
   "source": [
    "hyperparams = hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T05:54:15.321069Z",
     "iopub.status.busy": "2021-11-12T05:54:15.320437Z",
     "iopub.status.idle": "2021-11-12T05:54:15.376689Z",
     "shell.execute_reply": "2021-11-12T05:54:15.375449Z",
     "shell.execute_reply.started": "2021-11-12T05:54:15.32103Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available gpu count: 1\n",
      "<torch.cuda.device object at 0x7f2865d5bc70>\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(hyperparams.seed)\n",
    "print(\"available gpu count:\", torch.cuda.device_count())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.device(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T05:58:34.690798Z",
     "iopub.status.busy": "2021-11-12T05:58:34.69047Z",
     "iopub.status.idle": "2021-11-12T05:58:44.331666Z",
     "shell.execute_reply": "2021-11-12T05:58:44.330609Z",
     "shell.execute_reply.started": "2021-11-12T05:58:34.690766Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(hyperparams.tokenizer_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingredients (data) for Chaii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T05:56:31.067471Z",
     "iopub.status.busy": "2021-11-12T05:56:31.067157Z",
     "iopub.status.idle": "2021-11-12T05:56:31.851405Z",
     "shell.execute_reply": "2021-11-12T05:56:31.850328Z",
     "shell.execute_reply.started": "2021-11-12T05:56:31.067428Z"
    }
   },
   "outputs": [],
   "source": [
    "chaii_df = pd.read_csv(hyperparams.train_csv, encoding='utf-8')\n",
    "# sample_df = pd.read_csv('sample_submission.csv')\n",
    "# chaii_df = sklearn.utils.shuffle(chaii_df, random_state=4).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T05:56:53.337931Z",
     "iopub.status.busy": "2021-11-12T05:56:53.337284Z",
     "iopub.status.idle": "2021-11-12T05:56:53.348779Z",
     "shell.execute_reply": "2021-11-12T05:56:53.347394Z",
     "shell.execute_reply.started": "2021-11-12T05:56:53.337894Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df, val_df = model_selection.train_test_split(chaii_df, test_size=0.2, random_state=4) # hyperparams.seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T05:57:59.513966Z",
     "iopub.status.busy": "2021-11-12T05:57:59.513673Z",
     "iopub.status.idle": "2021-11-12T05:57:59.52053Z",
     "shell.execute_reply": "2021-11-12T05:57:59.519341Z",
     "shell.execute_reply.started": "2021-11-12T05:57:59.513935Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T05:58:21.763158Z",
     "iopub.status.busy": "2021-11-12T05:58:21.762678Z",
     "iopub.status.idle": "2021-11-12T05:58:21.782957Z",
     "shell.execute_reply": "2021-11-12T05:58:21.781841Z",
     "shell.execute_reply.started": "2021-11-12T05:58:21.763125Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_chaii(data_df, tokenizer, test=False):\n",
    "    # prepare_chaii takes in raw data and returns tokenized data \n",
    "    # along with position of first token and last token in the answer_text\n",
    "    \n",
    "    # strip trailing and leading whitespaces in context, question, and (answer_text)?\n",
    "    data_df.loc[:, 'context'] = data_df.loc[:, 'context'].apply(lambda sen : str(sen).strip())\n",
    "    data_df.loc[:, 'question'] = data_df.loc[:, 'question'].apply(lambda sen : str(sen).strip())\n",
    "    if not test:\n",
    "        data_df.loc[:, 'answer_text'] = data_df.loc[:, 'answer_text'].apply(lambda sen : str(sen).strip())\n",
    "    \n",
    "    # question; context -- order is important, and is used in prediction stage to find whether predicted tokens seq_id is 0 (question) or 1 (context)\n",
    "    data_tok = tokenizer(\n",
    "        list(data_df['question']), list(data_df['context']),\n",
    "        max_length=hyperparams.max_len, \n",
    "        truncation='only_second',\n",
    "        stride=hyperparams.overlap_len,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=True,\n",
    "    )\n",
    "    \n",
    "    if test:\n",
    "        return data_tok\n",
    "    \n",
    "    # data_df contains original raw data having question, context\n",
    "    # data_tok contains tokenized data, where context might have split into multiple sentences \n",
    "    # data_tok is a dict, containing keys : dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])\n",
    "    # every value is a list, and no tensors here\n",
    "    \n",
    "    # adding two more keys that will contain the position of first token and last token in the answer_text\n",
    "    data_tok['start_positions'], data_tok['end_positions'] = [], []\n",
    "    \n",
    "    n_sents = len(data_tok['input_ids'])\n",
    "    map_id_sent2context = data_tok['overflow_to_sample_mapping'] # id means index! since input_ids means various inputs to the model\n",
    "    map_offsets = data_tok['offset_mapping']\n",
    "    assert len(map_offsets) == len(map_id_sent2context) == n_sents\n",
    "    \n",
    "    for input_id in range(n_sents):\n",
    "        sent = data_tok['input_ids'][input_id]\n",
    "        \n",
    "        # get the answer_start and answer_text for this input_id using the id in data_df\n",
    "        context_id = map_id_sent2context[input_id]\n",
    "        answer_text = data_df.loc[context_id, 'answer_text']\n",
    "        answer_start = data_df.loc[context_id, 'answer_start']\n",
    "        answer_end = answer_start + len(answer_text) # will use this in next code block\n",
    "        \n",
    "        # check whether the answer is present in the current input_id or not using offsets\n",
    "        qn_context_id = data_tok.sequence_ids(input_id)\n",
    "        \n",
    "            # first: get the start_idx_token and end_idx_token of context\n",
    "        start_idx_token = qn_context_id.index(1)\n",
    "        end_idx_token = len(qn_context_id) - qn_context_id[::-1].index(1) - 1\n",
    "        \n",
    "            # second: use the offsets for input_id to find if answer_start and answer_end are inside this chunk of context or not\n",
    "        offset_map = map_offsets[input_id]\n",
    "\n",
    "        if answer_start >= offset_map[start_idx_token][0] and answer_end <= offset_map[end_idx_token][1]:\n",
    "            # now finally get the idx_token for the first and last token in the answer_text\n",
    "            while start_idx_token < len(sent) and answer_start >= offset_map[start_idx_token][0]:\n",
    "                start_idx_token += 1\n",
    "            while answer_end <= offset_map[end_idx_token][1]:\n",
    "                    end_idx_token -= 1\n",
    "            \n",
    "            data_tok['start_positions'].append(start_idx_token - 1)\n",
    "            data_tok['end_positions'].append(end_idx_token + 1)\n",
    "        \n",
    "        else:\n",
    "            cls_token_idx = sent.index(tokenizer.cls_token_id)\n",
    "            assert cls_token_idx == 0\n",
    "            data_tok['start_positions'].append(0) # cls token index\n",
    "            data_tok['end_positions'].append(0) # cls token index\n",
    "\n",
    "    return data_tok     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T05:58:25.414727Z",
     "iopub.status.busy": "2021-11-12T05:58:25.414422Z",
     "iopub.status.idle": "2021-11-12T05:58:25.425783Z",
     "shell.execute_reply": "2021-11-12T05:58:25.424434Z",
     "shell.execute_reply.started": "2021-11-12T05:58:25.414696Z"
    }
   },
   "outputs": [],
   "source": [
    "class chaii_ka_data(Dataset):\n",
    "    def __init__(self, data_df, tokenizer, test=False):\n",
    "        super(chaii_ka_data, self).__init__()\n",
    "        '''\n",
    "            test = True means data_df without answer_text, answer_start\n",
    "            data_df is the pandas dataframe containing context, question, ...        \n",
    "        '''\n",
    "        \n",
    "        # tokenize data samples context;question, and create new samples if overflow\n",
    "        # we need to do this apriori (and not in __getitem__ directly) because a datasample may create more samples upon tokenization\n",
    "        self.reqd_keys = ['input_ids', 'attention_mask'] \n",
    "        if not test:\n",
    "            self.reqd_keys += ['start_positions', 'end_positions']\n",
    "        self.data_tok = prepare_chaii(data_df, tokenizer, test=test)\n",
    "    \n",
    "    def __getitem__(self, input_id): # index is input_id as used in prepare_chaii()\n",
    "        # sent = self.data_tok['input_ids'][input_id]\n",
    "        # att_mask = self.data_tok['attention_mask'][input_id]\n",
    "        # offset_map = self.data_tok['offset_mapping'][input_id]\n",
    "        # start_idx_tok = self.data_tok['start_positions'][input_id]\n",
    "        # end_idx_tok = self.data_tok['end_positions'][input_id]\n",
    "        \n",
    "        return {k: torch.tensor(v[input_id], dtype=torch.long) for k,v in self.data_tok.items() if k in self.reqd_keys}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_tok['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T06:00:30.943333Z",
     "iopub.status.busy": "2021-11-12T06:00:30.94293Z",
     "iopub.status.idle": "2021-11-12T06:01:32.068092Z",
     "shell.execute_reply": "2021-11-12T06:01:32.06704Z",
     "shell.execute_reply.started": "2021-11-12T06:00:30.943289Z"
    }
   },
   "outputs": [],
   "source": [
    "trainset = chaii_ka_data(train_df, tokenizer)\n",
    "valset = chaii_ka_data(val_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model predictions (start and token index) to answer_text\n",
    "this transformation requires original (raw) data_df, data_tok, and start, end logits (probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serve_chaii(test_df, testset, logits):\n",
    "    assert len(logits[0]) == len(logits[1]) == len(testset)\n",
    "    submission = {\n",
    "        \"id\" : [],\n",
    "        \"PredictionString\" : []\n",
    "    }\n",
    "    n_examples = len(test_df)\n",
    "    # print(\"number of examples in test df\", n_examples)\n",
    "    for example_idx in range(n_examples):\n",
    "        # current example (or context) in the given test_df\n",
    "        example_id = test_df.loc[example_idx, 'id']\n",
    "\n",
    "        # get all the features (or sents), start_logits, end_logits for the current example index\n",
    "        data_tok = testset.data_tok\n",
    "        map_id_sent2context = data_tok['overflow_to_sample_mapping']\n",
    "        assert len(map_id_sent2context) == len(testset)\n",
    "\n",
    "        sents_first_idx = map_id_sent2context.index(example_idx)\n",
    "        sents_last_idx = len(map_id_sent2context) - map_id_sent2context[::-1].index(example_idx) - 1\n",
    "        assert (np.array(map_id_sent2context[sents_first_idx: sents_last_idx+1]) == example_idx).mean() == 1, set_trace()\n",
    "\n",
    "        sents = data_tok['input_ids'][sents_first_idx: sents_last_idx+1]\n",
    "        start_logits = logits[0][sents_first_idx: sents_last_idx+1]\n",
    "        end_logits = logits[1][sents_first_idx: sents_last_idx+1]\n",
    "        n_sents = len(sents)\n",
    "        assert n_sents == len(start_logits) == len(end_logits)\n",
    "\n",
    "        # get the answer_text from these sents using start_logits, end_logits\n",
    "        # rank all possible answers for each sentence\n",
    "        # then club all these answers from each sentence and take the best one as final predicted_answer\n",
    "        # Also, consider the case when a sentence has no answer_text i.e. model predicts no answer\n",
    "        pred_answers = []\n",
    "        for local_idx in range(n_sents):\n",
    "            sent = sents[local_idx]\n",
    "            start_lgts, end_lgts = start_logits[local_idx], end_logits[local_idx] # 384-dim list containing probabilities\n",
    "\n",
    "            # take the top 5 confident predictions of the model for start and end token indices\n",
    "            top_x = hyperparams.top_x\n",
    "            ranked_strt_tok_idxs = np.argsort(start_lgts)[::-1][:top_x].tolist()\n",
    "            ranked_end_tok_idxs = np.argsort(end_lgts)[::-1][:top_x].tolist()\n",
    "\n",
    "            # see which all are possible answers, and append\n",
    "            for start_tok_idx in ranked_strt_tok_idxs:\n",
    "                for end_tok_idx in ranked_end_tok_idxs:\n",
    "                    # meaningless prediction\n",
    "                    if start_tok_idx > end_tok_idx:\n",
    "                        continue\n",
    "\n",
    "                    # answer tokens NOT present in context, but in question\n",
    "                    seq_ids = data_tok.sequence_ids(sents_first_idx + local_idx)\n",
    "                    if seq_ids[start_tok_idx] == 0 or seq_ids[end_tok_idx] == 0: # 0 denotes question since question; context\n",
    "                        continue\n",
    "                        \n",
    "                    if end_tok_idx-start_tok_idx+1 > hyperparams.max_tok_in_ans:\n",
    "                        continue\n",
    "\n",
    "                    score = start_lgts[start_tok_idx] * end_lgts[end_tok_idx]\n",
    "                    answer_text = tokenizer.decode(sent[start_tok_idx: end_tok_idx+1])\n",
    "                    pred_answers.append((score, answer_text))\n",
    "\n",
    "        if len(pred_answers) == 0:\n",
    "            predicted_answer = \"\"\n",
    "\n",
    "        pred_answers = sorted(pred_answers, key=lambda element : element[0])[::-1]\n",
    "        predicted_answer = pred_answers[0][1]\n",
    "\n",
    "        if predicted_answer == \"<s>\":\n",
    "            predicted_answer = \"\"\n",
    "\n",
    "        submission['id'].append(example_id)\n",
    "        submission['PredictionString'].append(predicted_answer)\n",
    "\n",
    "    assert len(submission['id']) == len(test_df)\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T06:02:15.111523Z",
     "iopub.status.busy": "2021-11-12T06:02:15.111117Z",
     "iopub.status.idle": "2021-11-12T06:04:01.209031Z",
     "shell.execute_reply": "2021-11-12T06:04:01.207832Z",
     "shell.execute_reply.started": "2021-11-12T06:02:15.111473Z"
    }
   },
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(hyperparams.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T06:11:36.122121Z",
     "iopub.status.busy": "2021-11-12T06:11:36.120772Z",
     "iopub.status.idle": "2021-11-12T06:28:27.618494Z",
     "shell.execute_reply": "2021-11-12T06:28:27.616658Z",
     "shell.execute_reply.started": "2021-11-12T06:11:36.122055Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 11783\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7365\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7365' max='7365' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7365/7365 30:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.506000</td>\n",
       "      <td>0.420224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.263400</td>\n",
       "      <td>0.372058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.492951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.112300</td>\n",
       "      <td>0.496931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.061800</td>\n",
       "      <td>0.556646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2958\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to xlm-chaii/checkpoint-1473\n",
      "Configuration saved in xlm-chaii/checkpoint-1473/config.json\n",
      "Model weights saved in xlm-chaii/checkpoint-1473/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2958\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to xlm-chaii/checkpoint-2946\n",
      "Configuration saved in xlm-chaii/checkpoint-2946/config.json\n",
      "Model weights saved in xlm-chaii/checkpoint-2946/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2958\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to xlm-chaii/checkpoint-4419\n",
      "Configuration saved in xlm-chaii/checkpoint-4419/config.json\n",
      "Model weights saved in xlm-chaii/checkpoint-4419/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2958\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to xlm-chaii/checkpoint-5892\n",
      "Configuration saved in xlm-chaii/checkpoint-5892/config.json\n",
      "Model weights saved in xlm-chaii/checkpoint-5892/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2958\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to xlm-chaii/checkpoint-7365\n",
      "Configuration saved in xlm-chaii/checkpoint-7365/config.json\n",
      "Model weights saved in xlm-chaii/checkpoint-7365/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7365, training_loss=0.22013820122474728, metrics={'train_runtime': 1813.173, 'train_samples_per_second': 32.493, 'train_steps_per_second': 4.062, 'total_flos': 1.154572381732608e+16, 'train_loss': 0.22013820122474728, 'epoch': 5.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=hyperparams.output_dir, \n",
    "    overwrite_output_dir=True, \n",
    "    per_device_eval_batch_size=hyperparams.batch_size,\n",
    "    per_device_train_batch_size=hyperparams.batch_size,\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=hyperparams.epochs,\n",
    "    seed=hyperparams.seed,\n",
    "    save_strategy=\"epoch\",\n",
    "#     save_total_limit=1,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"eval_accuracy\",\n",
    "#     greater_is_better=True,\n",
    "#     warmup_steps=500,\n",
    "#     logging_dir='./logs',\n",
    "#     logging_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args,\n",
    "    train_dataset=trainset, eval_dataset=valset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Compute Jaccard's score for trainset, valset using saved model at each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T06:53:44.664947Z",
     "iopub.status.busy": "2021-11-12T06:53:44.664314Z",
     "iopub.status.idle": "2021-11-12T06:53:44.680103Z",
     "shell.execute_reply": "2021-11-12T06:53:44.677732Z",
     "shell.execute_reply.started": "2021-11-12T06:53:44.664913Z"
    }
   },
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jaccard(pred_df, gt_df):\n",
    "    num_examples = 0\n",
    "    score = 0\n",
    "    for idx, example_id in enumerate(gt_df.loc[:,'id']):\n",
    "        gt_answer = gt_df.loc[idx, 'answer_text']\n",
    "        pred_answer = pred_df.loc[pred_df.loc[:, 'id'] == example_id].reset_index(drop=True).loc[0, 'PredictionString']\n",
    "        # print(gt_answer, pred_answer)\n",
    "        score += jaccard(gt_answer, pred_answer)\n",
    "        num_examples += 1\n",
    "        \n",
    "    score /= num_examples\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T07:09:27.283202Z",
     "iopub.status.busy": "2021-11-12T07:09:27.282907Z",
     "iopub.status.idle": "2021-11-12T07:09:49.342984Z",
     "shell.execute_reply": "2021-11-12T07:09:49.341152Z",
     "shell.execute_reply.started": "2021-11-12T07:09:27.283173Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file xlm-chaii/checkpoint-1473/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"deepset/xlm-roberta-base-squad2\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"language\": \"english\",\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"name\": \"XLMRoberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file xlm-chaii/checkpoint-1473/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForQuestionAnswering.\n",
      "\n",
      "All the weights of XLMRobertaForQuestionAnswering were initialized from the model checkpoint at xlm-chaii/checkpoint-1473.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForQuestionAnswering for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 11783\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1473' max='1473' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1473/1473 01:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file xlm-chaii/checkpoint-2946/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"deepset/xlm-roberta-base-squad2\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"language\": \"english\",\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"name\": \"XLMRoberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file xlm-chaii/checkpoint-2946/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForQuestionAnswering.\n",
      "\n",
      "All the weights of XLMRobertaForQuestionAnswering were initialized from the model checkpoint at xlm-chaii/checkpoint-2946.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForQuestionAnswering for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 11783\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1473' max='1473' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1473/1473 01:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file xlm-chaii/checkpoint-4419/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"deepset/xlm-roberta-base-squad2\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"language\": \"english\",\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"name\": \"XLMRoberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file xlm-chaii/checkpoint-4419/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForQuestionAnswering.\n",
      "\n",
      "All the weights of XLMRobertaForQuestionAnswering were initialized from the model checkpoint at xlm-chaii/checkpoint-4419.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForQuestionAnswering for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 11783\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1473' max='1473' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1473/1473 01:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file xlm-chaii/checkpoint-5892/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"deepset/xlm-roberta-base-squad2\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"language\": \"english\",\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"name\": \"XLMRoberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file xlm-chaii/checkpoint-5892/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForQuestionAnswering.\n",
      "\n",
      "All the weights of XLMRobertaForQuestionAnswering were initialized from the model checkpoint at xlm-chaii/checkpoint-5892.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForQuestionAnswering for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 11783\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1473' max='1473' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1473/1473 01:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file xlm-chaii/checkpoint-7365/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"deepset/xlm-roberta-base-squad2\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"language\": \"english\",\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"name\": \"XLMRoberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file xlm-chaii/checkpoint-7365/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForQuestionAnswering.\n",
      "\n",
      "All the weights of XLMRobertaForQuestionAnswering were initialized from the model checkpoint at xlm-chaii/checkpoint-7365.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForQuestionAnswering for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 11783\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1473' max='1473' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1473/1473 01:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['checkpoint-1473', 'checkpoint-2946', 'checkpoint-4419', 'checkpoint-5892', 'checkpoint-7365', 'runs']\n",
      "[0.09774824564386851, 0.3536411291040921, 0.636208887134813, 0.7312489979156648, 0.745868740313185]\n"
     ]
    }
   ],
   "source": [
    "model_checkpoints = os.listdir(hyperparams.output_dir)\n",
    "model_checkpoints.sort()\n",
    "train_scores, val_scores = [], []\n",
    "for cp_id, model_checkpoint in enumerate(model_checkpoints):\n",
    "    if model_checkpoint[:5] != \"check\":\n",
    "        continue\n",
    "    # load the model\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(os.path.join(hyperparams.output_dir, model_checkpoint))\n",
    "    trainer = Trainer(\n",
    "        model=model\n",
    "    )\n",
    "    \n",
    "    logits = trainer.predict(trainset).predictions\n",
    "    submission = serve_chaii(train_df, trainset, logits)\n",
    "    pred_df = pd.DataFrame(submission)\n",
    "    train_score = compute_jaccard(pred_df, train_df)\n",
    "    train_scores.append(train_score)\n",
    "\n",
    "    # logits = trainer.predict(valset).predictions\n",
    "    # submission = serve_chaii(val_df, valset, logits)\n",
    "    # pred_df = pd.DataFrame(submission)\n",
    "    # val_score = compute_jaccard(pred_df, val_df)\n",
    "    # val_scores.append(val_score)\n",
    "    \n",
    "    \n",
    "    # print(model_checkpoint, \"train:\", train_score, \"val:\", val_score)\n",
    "print(model_checkpoints)\n",
    "print(train_scores)\n",
    "# print(val_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction of textual answers\n",
    "1. generate submission.csv containing \"id\", \"PredictionString\" columns\n",
    "2. Use Trainer API for predict instead of trainer.model(\\*\\*batch) as it handles batching, and CPU-GPU on its own\n",
    "3. trainer.predict(testset) gives the start and end logits for all the input features in the test set\n",
    "4. for each example in the test_df, select the best answer from its features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T06:28:39.062271Z",
     "iopub.status.busy": "2021-11-12T06:28:39.061929Z",
     "iopub.status.idle": "2021-11-12T06:28:57.878706Z",
     "shell.execute_reply": "2021-11-12T06:28:57.877386Z",
     "shell.execute_reply.started": "2021-11-12T06:28:39.062203Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"./ShAm-ki-chaii/checkpoint-1473\")\n",
    "trainer = Trainer(\n",
    "    model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T06:53:39.856846Z",
     "iopub.status.busy": "2021-11-12T06:53:39.856552Z",
     "iopub.status.idle": "2021-11-12T06:53:41.046855Z",
     "shell.execute_reply": "2021-11-12T06:53:41.044917Z",
     "shell.execute_reply.started": "2021-11-12T06:53:39.856814Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "# test_df = pd.read_csv(hyperparams.test_csv, encoding='utf-8') # uncomment this for submission\n",
    "test_df = pd.read_csv(hyperparams.train_csv, encoding='utf-8').loc[:10] # comment this for submission\n",
    "testset = chaii_ka_data(test_df, tokenizer, test=True)\n",
    "# testloader = DataLoader(testset, batch_size=16)\n",
    "# next(iter(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T06:53:41.053867Z",
     "iopub.status.busy": "2021-11-12T06:53:41.053629Z",
     "iopub.status.idle": "2021-11-12T06:53:41.062699Z",
     "shell.execute_reply": "2021-11-12T06:53:41.060918Z",
     "shell.execute_reply.started": "2021-11-12T06:53:41.053837Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T06:53:41.123504Z",
     "iopub.status.busy": "2021-11-12T06:53:41.122687Z",
     "iopub.status.idle": "2021-11-12T06:53:43.963862Z",
     "shell.execute_reply": "2021-11-12T06:53:43.96238Z",
     "shell.execute_reply.started": "2021-11-12T06:53:41.123465Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 143\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pass the complete testset in trainer API\n",
    "# the API will automatically do batch-wise prediction\n",
    "# start, end logits are accessible using model_output.predictions[0],[1]\n",
    "# if the testset has labels, then model_output.label_ids contains them\n",
    "# model_output.metrics = {'test_runtime': 10.6385, 'test_samples_per_second': 128.684, 'test_steps_per_second': 16.168}\n",
    "model_output = trainer.predict(testset) \n",
    "logits = model_output.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T06:53:44.64549Z",
     "iopub.status.busy": "2021-11-12T06:53:44.645185Z",
     "iopub.status.idle": "2021-11-12T06:53:44.658656Z",
     "shell.execute_reply": "2021-11-12T06:53:44.657206Z",
     "shell.execute_reply.started": "2021-11-12T06:53:44.645461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples in test df 11\n"
     ]
    }
   ],
   "source": [
    "submission = serve_chaii(test_df, testset, logits)\n",
    "submission_df = pd.DataFrame(submission)\n",
    "# submission_df.loc[:, 'PredictionString'] = submission_df.loc[:, 'PredictionString'].apply(lambda ans: \"\\\"\" + str(ans) + \"\\\"\")\n",
    "submission\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### references\n",
    "1. https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb\n",
    "2. https://huggingface.co/transformers/internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.__call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T06:53:44.687457Z",
     "iopub.status.busy": "2021-11-12T06:53:44.687138Z",
     "iopub.status.idle": "2021-11-12T06:53:44.703753Z",
     "shell.execute_reply": "2021-11-12T06:53:44.702434Z",
     "shell.execute_reply.started": "2021-11-12T06:53:44.68742Z"
    }
   },
   "outputs": [],
   "source": [
    "# pred_df = pd.read_csv('submission.csv', encoding='utf-8')\n",
    "# gt_df = test_df # pd.read_csv(hyperparams.val_csv, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T06:53:48.312177Z",
     "iopub.status.busy": "2021-11-12T06:53:48.311889Z",
     "iopub.status.idle": "2021-11-12T06:53:48.325604Z",
     "shell.execute_reply": "2021-11-12T06:53:48.32397Z",
     "shell.execute_reply.started": "2021-11-12T06:53:48.312147Z"
    }
   },
   "outputs": [],
   "source": [
    "# pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T06:53:50.564968Z",
     "iopub.status.busy": "2021-11-12T06:53:50.563943Z",
     "iopub.status.idle": "2021-11-12T06:53:50.57725Z",
     "shell.execute_reply": "2021-11-12T06:53:50.575593Z",
     "shell.execute_reply.started": "2021-11-12T06:53:50.56493Z"
    }
   },
   "outputs": [],
   "source": [
    "# num_examples = len(pred_df)\n",
    "# print(\"number of predictions\", num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T06:59:33.871922Z",
     "iopub.status.busy": "2021-11-12T06:59:33.871642Z",
     "iopub.status.idle": "2021-11-12T06:59:33.899606Z",
     "shell.execute_reply": "2021-11-12T06:59:33.897237Z",
     "shell.execute_reply.started": "2021-11-12T06:59:33.871893Z"
    }
   },
   "outputs": [],
   "source": [
    "# score = 0\n",
    "# for idx, example_id in enumerate(gt_df.loc[:,'id']):\n",
    "#     gt_answer = gt_df.loc[idx, 'answer_text']\n",
    "#     pred_answer = pred_df.loc[pred_df.loc[:, 'id'] == example_id].reset_index(drop=True).loc[0, 'PredictionString']\n",
    "#     # print(gt_answer, pred_answer)\n",
    "#     score += jaccard(gt_answer, pred_answer)\n",
    "# score /= num_examples\n",
    "# print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
